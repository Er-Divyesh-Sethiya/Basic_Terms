# Basic_terms
To define and create Neural Network

1. Model Deployment: 
- Inference: The process of using a trained model to make predictions or generate outputs on new, unseen data. 
- Model Serialization: Saving the trained model's parameters and architecture to a file format for future deployment and use. 
- Model Compression: Techniques to reduce the size of the trained model for efficient storage and deployment on resource-constrained devices. 
2. Transfer Learning Techniques: 
- Fine-tuning: Updating the parameters of a pre-trained model on a new task or dataset by further training it with the new data. 
- Feature Extraction: Using the pre-trained model as a fixed feature extractor and training a separate classifier on top of its extracted features. 
4. Interpretability and Explainability: 
- Grad-CAM: Gradient-weighted Class Activation Mapping, a technique to visualize the regions of an image that contribute most to the model's prediction. 
- SHAP (Shapley Additive Explanations): An approach to explain the predictions of complex models by assigning importance values to the input features. 
5. Generative Adversarial Networks (GANs): 
- Generator: A network that generates new samples by mapping random input vectors to the desired data distribution. 
- Discriminator: A network that distinguishes between real and generated samples, providing feedback to the generator for training. 
6. Self-Supervised Learning: 
- Pretext Task: A proxy task designed to learn useful representations from unlabeled data, which can later be fine-tuned for downstream tasks. 
- Contrastive Learning: A self-supervised learning approach that trains the model to differentiate positive pairs from negative pairs in the latent space. 
7. Autoencoders: 
- Encoder: A network that compresses the input data into a low-dimensional representation. 
- Decoder: A network that reconstructs the input data from the encoded representation, aiming to minimize the reconstruction error.
